title: "Language Modeling with Transformer"
trainer: "LMTrainer"
random_seed: 42
val_freq_iters: 1000
# logging:
#   training_progress: False
checkpoint:
  freq_epochs: 5
  pickle: ["vocab"]
modules:
  models: ["model"]
  optims: ["optim"]
data:
  train: "data/classics.split.tok.5_50.common.bpe"
  # train: "data/generated/classics.split.tok.5_50.common.bpe.tiny-val"
  val: "data/generated/classics.split.tok.5_50.common.bpe.tiny-val"
hp:
  batch_size: 200
  batch_len: 128
  transformer:
    d_model: 512
    num_layers: 6
    max_seq_len: 512
