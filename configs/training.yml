title: "Language Modeling with Transformer"
trainer: "LMTrainer"
random_seed: 42
val_freq_epochs: 10
num_val_examples_to_display: 100
data:
  train: "data/classics.split.tok.5_50.common.bpe"
  val: "data/generated/classics.split.tok.5_50.common.bpe.tiny-val"
hp:
  batch_size: 64
  batch_len: 128
  transformer:
    d_model: 512
    num_layers: 6
    max_seq_len: 512
